{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IGENIERÍA DE DATOS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INGESTA DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importo la librería pandas y gzip (cabe aclarar que se debe tener instalado pandas previamente, para eso utilizo el comando pip install pandas)\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decido utilizar los 3 archivos directamente en formato comprimido, para el caso del archivo \"steam_games\", se puede leer directamente utilizando\n",
    "gzip pero para los siguientes 2 archivos: \"user_reviews\" y \"user_items\" hubo que hacer una función ya que el json venía con comillas simples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ingesta de datos del primer archivo \"steam_games\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#con gzip abrimos el json y generamos un dataframe\n",
    "with gzip.open('/Users/gaston/Documents/Carrera Data Analytics/Proyectos Individuales/Proyecto 1/steam_games.json.gz', 'rt', encoding='utf-8') as steam_games:\n",
    "       \n",
    "    df_steam_games = pd.read_json(steam_games, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "steam_games = df_steam_games.drop(columns = ['items','user_id','steam_id','items_count','tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "steam_games.dropna(inplace = True, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#realizo una función para separar la columna \"release_date\" en year, month y day para luego consumir en las funciones\n",
    "def procesar_fecha(fecha):\n",
    "    if fecha and fecha != 'none':\n",
    "        partes = fecha.split(\"-\")\n",
    "        if len(partes) == 3:\n",
    "            año = partes[0]\n",
    "            mes = partes[1]\n",
    "            dia = partes[2]\n",
    "            return año, mes, dia\n",
    "    return None, None, None\n",
    "\n",
    "# Aplicp la función a la columna \"release_date\" y creo columnas separadas para año, mes y día\n",
    "steam_games[['release_year', 'release_month', 'release_day']] = steam_games['release_date'].apply(lambda x: pd.Series(procesar_fecha(x)))\n",
    "\n",
    "# Elimino la columna original \"release_date\"\n",
    "steam_games.drop(columns=['release_date'], inplace=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ingesta de datos del archivo \"user_reviews\" obteniendo un df_reviews_desanidado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora genero un dataframe para \"user_reviews\" utilizando una función que recorre y completa una lista vacía con la cual genero el DF\n",
    "import ast #importo la librería ast a utilizar\n",
    "\n",
    "info = [] \n",
    "\n",
    "for i in gzip.open('/Users/gaston/Documents/Carrera Data Analytics/Proyectos Individuales/Proyecto 1/user_reviews.json.gz'):\n",
    "     info.append(ast.literal_eval(i.decode('utf-8')))\n",
    "     \n",
    "df_user_reviews = pd.DataFrame(info) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con las funciones explode y normalize desanidamos las columna \"reviews\" para poder\n",
    "# disponibilizar esos datos para consumirlos en etapas posteriores\n",
    "df_reviews_exploded = df_user_reviews.explode('reviews')\n",
    "df_reviews_desanidado = pd.json_normalize(df_reviews_exploded['reviews'].dropna())\n",
    "\n",
    "# Por otra parte reindexamos los DF\n",
    "df_reviews_desanidado.reset_index(inplace=True)\n",
    "df_reviews_exploded.reset_index(inplace=True)\n",
    "\n",
    "# Concatenamos y eliminamosla columna que originalmente desanidamos\n",
    "\n",
    "user_reviews = pd.concat([df_reviews_exploded,df_reviews_desanidado], axis=1)\n",
    "user_reviews = user_reviews.drop(columns = ['reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verifico y borro los duplicados\n",
    "user_reviews_duplicados =  user_reviews.duplicated( keep=\"first\")\n",
    "user_reviews_duplicados.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_reviews = user_reviews.drop_duplicates(keep = \"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#realizo una función para separar la columna \"posted\" en dia, mes y año para luego consumir en las funciones\n",
    "\n",
    "def procesar_fecha(data):\n",
    "    import re\n",
    "    if isinstance(data, str):\n",
    "        match = re.search(r'(\\w+) (\\d+), (\\d+)', data)\n",
    "        if match:\n",
    "            mes = match.group(1)\n",
    "            dia = match.group(2)\n",
    "            año = match.group(3)\n",
    "            meses = {\n",
    "                'January': '01',\n",
    "                'February': '02',\n",
    "                'March': '03',\n",
    "                'April': '04',\n",
    "                'May': '05',\n",
    "                'June': '06',\n",
    "                'July': '07',\n",
    "                'August': '08',\n",
    "                'September': '09',\n",
    "                'October': '10',\n",
    "                'November': '11',\n",
    "                'December': '12'\n",
    "            }\n",
    "            mes_numero = meses.get(mes)\n",
    "            if mes_numero is not None:\n",
    "                return mes_numero, dia, año\n",
    "    return None, None, None\n",
    "\n",
    "# Aplico la función a la columna \"posted\" después de convertirla en cadenas de texto y unifico la fecha en el formato\n",
    "#YYYY-MM-DD para consumir en las funciones\n",
    "user_reviews[['Mes', 'Día', 'Año']] = user_reviews['posted'].astype(str).apply(lambda x: pd.Series(procesar_fecha(x)))\n",
    "user_reviews['Año'].fillna(0, inplace=True)\n",
    "user_reviews['Mes'].fillna(0, inplace=True)\n",
    "user_reviews['Día'].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "# Convierte las columnas 'Año', 'Mes' y 'Día' en enteros\n",
    "user_reviews['Año'] = user_reviews['Año'].astype(int)\n",
    "user_reviews['Año'] = user_reviews['Año'].fillna(0).astype(int)\n",
    "user_reviews['Mes'] = user_reviews['Mes'].astype(int)\n",
    "user_reviews['Mes'] = user_reviews['Mes'].fillna(0).astype(int)\n",
    "user_reviews['Día'] = user_reviews['Día'].astype(int)\n",
    "user_reviews['Día'] = user_reviews['Día'].fillna(0).astype(int)\n",
    "# Define una fecha predeterminada para las filas con valores cero\n",
    "fecha_predeterminada = pd.to_datetime('1900-01-01')\n",
    "\n",
    "# Crea una nueva columna 'fecha_review' con la fecha predeterminada para las filas con valores cero\n",
    "user_reviews['fecha_review'] = user_reviews.apply(lambda row: pd.to_datetime(f\"{row['Año']}-{row['Mes']}-{row['Día']}\", format='%Y-%m-%d', errors='coerce') if row['Año'] != 0 and row['Mes'] != 0 and row['Día'] != 0 else fecha_predeterminada, axis=1)\n",
    "\n",
    "# Elimino la columna original \"posted\"\n",
    "user_reviews.drop(columns=['posted'], inplace=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora voy a realizar el análisis de sentimiento con NLP utilizando la librería NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al ejecutar estos comandos, me encontré con errores por los certificados SSL, y tuve que instalar de otra forma como\n",
    "muestro en el siguiente Markdown\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defino la función para analizar los sentimientos\n",
    "def analizar_sentimiento(texto):\n",
    "    if isinstance(texto, str):\n",
    "        sia = SentimentIntensityAnalyzer()\n",
    "        sentimiento = sia.polarity_scores(texto)\n",
    "    \n",
    "        # Determinar la etiqueta en función de la puntuación compuesta\n",
    "        if sentimiento['compound'] >= 0.05:\n",
    "            return '2'\n",
    "        elif sentimiento['compound'] <= -0.05:\n",
    "            return '0'\n",
    "        else:\n",
    "            return '1'\n",
    "    else:\n",
    "            return '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_reviews['sentimiento'] = user_reviews['review'].apply(analizar_sentimiento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Borro registros con valores nulos en la columna \"review\" y duplicados\n",
    "user_reviews = user_reviews.dropna(subset=['review'])\n",
    "user_reviews = user_reviews.drop_duplicates( keep = \"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elimino la columma \"index\"\n",
    "user_reviews = user_reviews.drop(columns = ['index'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ingesta de datos del archivo \"user_items\" obteniendo un df_items_desanidado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora genero un dataframe para \"users_items\" utilizando una función que recorre y completa una lista vacía con la cual genero el DF\n",
    "import ast #importo la librería ast a utilizar\n",
    "\n",
    "info = [] \n",
    "\n",
    "for i in gzip.open('/Users/gaston/Documents/Carrera Data Analytics/Proyectos Individuales/Proyecto 1/users_items.json.gz'):\n",
    "     info.append(ast.literal_eval(i.decode('utf-8')))\n",
    "     \n",
    "df_users_items = pd.DataFrame(info) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con las funciones explode y normalize desanidamos las columna \"items\" para poder\n",
    "# disponibilizar esos datos para consumirlos en etapas posteriores\n",
    "df_items_exploded = df_users_items.explode('items')\n",
    "df_items_desanidado = pd.json_normalize(df_items_exploded['items'])\n",
    "\n",
    "# Por otra parte reindexamos los DF\n",
    "df_items_exploded.reset_index(inplace=True)\n",
    "df_items_desanidado.reset_index(inplace=True)\n",
    "\n",
    "# Concatenamos y eliminamos la columna que previamente desanidamos\n",
    "user_items = pd.concat([df_items_exploded,df_items_desanidado], axis=1)\n",
    "user_items = user_items.drop(columns = ['items'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#identifico valores duplicados, como no hay registros duplicados, no es necesario borrar\n",
    "user_items_duplicados =  user_items.duplicated( keep=\"first\")\n",
    "user_items_duplicados.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#borro los duplicados\n",
    "user_items = user_items.drop_duplicates(keep = \"first\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uno los datasets (DataFrames) user_items & steam_games ya que su info es complementaria y necesaria para las funciones\n",
    "que debemos armar pero primero cambio el tipo de dato a float para poder unirlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_items['item_id']=user_items['item_id'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "steam_games['item_id']=user_items['item_id'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_items_steam_games = user_items.merge(steam_games, left_on='item_id',right_on='id',how='left')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la columna precio, defino que si el valor no es numérico, sea 0 para evitar errores de cálculos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_items_steam_games['price'] = user_items_steam_games['price'].apply(lambda x: 0 if isinstance(x, str) else (x if pd.notna(x) else np.nan))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRATAMIENTO DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#borro las columnas \"index\" del dataframe user_items_steam_games\n",
    "user_items_steam_games = user_items_steam_games.drop(columns = ['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Al intentar exportar el df a parquet, encontré que en la columna \"metascore\" tenía valores \"none\" que\n",
    "#me generaban error ya que no es un Nan, por lo que decido reemplazarlos por NA\n",
    "\n",
    "# Reemplazo 'NA' con NaN en la columna 'metascore'\n",
    "user_items_steam_games['metascore'] = user_items_steam_games['metascore'].replace('NA', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decido borrar las columnas playtime_2weeks, specs y publisher para reducir espacio en el dataframe user_items_steam_games\n",
    "user_items_steam_games = user_items_steam_games.drop(columns = ['playtime_2weeks'])\n",
    "user_items_steam_games = user_items_steam_games.drop(columns = ['specs'])\n",
    "user_items_steam_games = user_items_steam_games.drop(columns = ['publisher'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de confeccionados los dataframes con la info que necesito para consumir en las funciones,\n",
    "decido guardarlos en archivos parquet y no en csv para optimizar "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para eso primero instalo la biblioteca pyarrow con el comando pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importo pyarrow\n",
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renombro los dataframes\n",
    "reviews= user_reviews\n",
    "users_games = user_items_steam_games\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la API tuve que instalar fastapi y uvicorn para poder crear el servidor web (consultar documentación en https://fastapi.tiangolo.com)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uno de los pedidos para este proyecto fue aplicar un análisis de sentimiento a los reviews de los usuarios. Para ello se creó una nueva columna llamada 'sentiment_analysis' que reemplaza a la columna que contiene los reviews donde clasifica los sentimientos de los comentarios con la siguiente escala:\n",
    "\n",
    "0 si es malo,\n",
    "1 si es neutral o esta sin review\n",
    "2 si es positivo.\n",
    "Dado que el objetivo de este proyecto es realizar una prueba de concepto, se realiza un análisis de sentimiento básico utilizando TextBlob que es una biblioteca de procesamiento de lenguaje natural (NLP) en Python. El objetivo de esta metodología es asignar un valor numérico a un texto, en este caso a los comentarios que los usuarios dejaron para un juego determinado, para representar si el sentimiento expresado en el texto es negativo, neutral o positivo.\n",
    "\n",
    "Esta metodología toma una revisión de texto como entrada, utiliza TextBlob para calcular la polaridad de sentimiento y luego clasifica la revisión como negativa, neutral o positiva en función de la polaridad calculada. En este caso, se consideraron las polaridades por defecto del modelo, el cuál utiliza umbrales -0.2 y 0.2, siendo polaridades negativas por debajo de -0.2, positivas por encima de 0.2 y neutrales entre medio de ambos.\n",
    "\n",
    "Por otra parte, y bajo el mismo criterio de optimizar los tiempos de respuesta de las consultas en la API y teniendo en cuenta las limitaciones de almacenamiento en el servicio de nube para deployar la API, se realizaron dataframes auxiliares para cada una de las funciones solicitadas. En el mismo sentido, se guardaron estos dataframes en formato parquet que permite una compresión y codificación eficiente de los datos.\n",
    "\n",
    "Todos los detalles del desarrollo se pueden ver en la Jupyter Notebook 01d_Feature_eng."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Levanto los archivos parquet exportados luego de la limpieza de datos realizada en la etapa de Data Engineer (consultar archivo Data Engineer.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#genero un archivo parquet con el dataframe \"reviews\" para utilizar en las funcione de la api\n",
    "reviews.to_parquet('user_reviews.parquet', engine='pyarrow', compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#genero un dataframe para consumir en la función userdata\n",
    "df_userdata = users_games[['user_id','price']]\n",
    "df_userdata.to_parquet('df_userdata.parquet', engine='pyarrow', compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#genero un dataframe para consumir en la función genre_ranking\n",
    "users_games_genero = users_games[['genres','playtime_forever']]\n",
    " #Realizo un explode de la columna 'genres' para poder desanidar la información en distintos registros\n",
    "users_games_generos = users_games_genero[['genres','playtime_forever']].explode('genres')\n",
    "    \n",
    "# Agrupo por género y sumo los valores de \"playtime_forever\"\n",
    "agrupado = users_games_generos.groupby('genres')['playtime_forever'].sum().reset_index()\n",
    "\n",
    "# Ordeno por la suma de \"playtime_forever\" en orden descendente\n",
    "agrupado = agrupado.sort_values(by='playtime_forever', ascending=False)\n",
    "agrupado.to_parquet('generos.parquet', engine='pyarrow', compression='snappy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#genero un nuevo dataframe con las columnas genre, playtime_forever, user_id y user_url. Lo exporto a parquet para luego consumirlo\n",
    "df_generos_horas = users_games[['user_id','user_url','genres','playtime_forever']]\n",
    "#Realizo un explode de la columna 'genres' para poder desanidar la información en distintos registros\n",
    "df_generos_horas = df_generos_horas.explode('genres')\n",
    "    \n",
    "# Agrupo por usuario y sumo las horas de juego para cada usuario\n",
    "df_generos_horas = df_generos_horas.groupby(['genres','user_id', 'user_url'])['playtime_forever'].sum().reset_index()\n",
    "\n",
    "df_generos_horas.to_parquet('generos_horas.parquet', engine='pyarrow', compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genres</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_url</th>\n",
       "      <th>playtime_forever</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Action</td>\n",
       "      <td>--000--</td>\n",
       "      <td>http://steamcommunity.com/id/--000--</td>\n",
       "      <td>139469.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Action</td>\n",
       "      <td>--ace--</td>\n",
       "      <td>http://steamcommunity.com/id/--ace--</td>\n",
       "      <td>69325.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Action</td>\n",
       "      <td>--ionex--</td>\n",
       "      <td>http://steamcommunity.com/id/--ionex--</td>\n",
       "      <td>38315.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Action</td>\n",
       "      <td>-2SV-vuLB-Kg</td>\n",
       "      <td>http://steamcommunity.com/id/-2SV-vuLB-Kg</td>\n",
       "      <td>42500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Action</td>\n",
       "      <td>-404PageNotFound-</td>\n",
       "      <td>http://steamcommunity.com/id/-404PageNotFound-</td>\n",
       "      <td>117423.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704777</th>\n",
       "      <td>Web Publishing</td>\n",
       "      <td>zepavil</td>\n",
       "      <td>http://steamcommunity.com/id/zepavil</td>\n",
       "      <td>37926.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704778</th>\n",
       "      <td>Web Publishing</td>\n",
       "      <td>zeshirky</td>\n",
       "      <td>http://steamcommunity.com/id/zeshirky</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704779</th>\n",
       "      <td>Web Publishing</td>\n",
       "      <td>zevlupine</td>\n",
       "      <td>http://steamcommunity.com/id/zevlupine</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704780</th>\n",
       "      <td>Web Publishing</td>\n",
       "      <td>zilaman</td>\n",
       "      <td>http://steamcommunity.com/id/zilaman</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704781</th>\n",
       "      <td>Web Publishing</td>\n",
       "      <td>zipper559</td>\n",
       "      <td>http://steamcommunity.com/id/zipper559</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>704782 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                genres            user_id   \n",
       "0               Action            --000--  \\\n",
       "1               Action            --ace--   \n",
       "2               Action          --ionex--   \n",
       "3               Action       -2SV-vuLB-Kg   \n",
       "4               Action  -404PageNotFound-   \n",
       "...                ...                ...   \n",
       "704777  Web Publishing            zepavil   \n",
       "704778  Web Publishing           zeshirky   \n",
       "704779  Web Publishing          zevlupine   \n",
       "704780  Web Publishing            zilaman   \n",
       "704781  Web Publishing          zipper559   \n",
       "\n",
       "                                              user_url  playtime_forever  \n",
       "0                 http://steamcommunity.com/id/--000--          139469.0  \n",
       "1                 http://steamcommunity.com/id/--ace--           69325.0  \n",
       "2               http://steamcommunity.com/id/--ionex--           38315.0  \n",
       "3            http://steamcommunity.com/id/-2SV-vuLB-Kg           42500.0  \n",
       "4       http://steamcommunity.com/id/-404PageNotFound-          117423.0  \n",
       "...                                                ...               ...  \n",
       "704777            http://steamcommunity.com/id/zepavil           37926.0  \n",
       "704778           http://steamcommunity.com/id/zeshirky               1.0  \n",
       "704779          http://steamcommunity.com/id/zevlupine               4.0  \n",
       "704780            http://steamcommunity.com/id/zilaman               9.0  \n",
       "704781          http://steamcommunity.com/id/zipper559               3.0  \n",
       "\n",
       "[704782 rows x 4 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_generos_horas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_url</th>\n",
       "      <th>playtime_forever</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--000--</td>\n",
       "      <td>http://steamcommunity.com/id/--000--</td>\n",
       "      <td>229119.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>--ace--</td>\n",
       "      <td>http://steamcommunity.com/id/--ace--</td>\n",
       "      <td>91837.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>--ionex--</td>\n",
       "      <td>http://steamcommunity.com/id/--ionex--</td>\n",
       "      <td>56735.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2SV-vuLB-Kg</td>\n",
       "      <td>http://steamcommunity.com/id/-2SV-vuLB-Kg</td>\n",
       "      <td>83813.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-404PageNotFound-</td>\n",
       "      <td>http://steamcommunity.com/id/-404PageNotFound-</td>\n",
       "      <td>477522.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87621</th>\n",
       "      <td>zzonci</td>\n",
       "      <td>http://steamcommunity.com/id/zzonci</td>\n",
       "      <td>1716.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87622</th>\n",
       "      <td>zzoptimuszz</td>\n",
       "      <td>http://steamcommunity.com/id/zzoptimuszz</td>\n",
       "      <td>432433.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87623</th>\n",
       "      <td>zzydrax</td>\n",
       "      <td>http://steamcommunity.com/id/zzydrax</td>\n",
       "      <td>9361.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87624</th>\n",
       "      <td>zzyfo</td>\n",
       "      <td>http://steamcommunity.com/id/zzyfo</td>\n",
       "      <td>86455.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87625</th>\n",
       "      <td>zzzmidmiss</td>\n",
       "      <td>http://steamcommunity.com/id/zzzmidmiss</td>\n",
       "      <td>21890.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87626 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 user_id                                        user_url   \n",
       "0                --000--            http://steamcommunity.com/id/--000--  \\\n",
       "1                --ace--            http://steamcommunity.com/id/--ace--   \n",
       "2              --ionex--          http://steamcommunity.com/id/--ionex--   \n",
       "3           -2SV-vuLB-Kg       http://steamcommunity.com/id/-2SV-vuLB-Kg   \n",
       "4      -404PageNotFound-  http://steamcommunity.com/id/-404PageNotFound-   \n",
       "...                  ...                                             ...   \n",
       "87621             zzonci             http://steamcommunity.com/id/zzonci   \n",
       "87622        zzoptimuszz        http://steamcommunity.com/id/zzoptimuszz   \n",
       "87623            zzydrax            http://steamcommunity.com/id/zzydrax   \n",
       "87624              zzyfo              http://steamcommunity.com/id/zzyfo   \n",
       "87625         zzzmidmiss         http://steamcommunity.com/id/zzzmidmiss   \n",
       "\n",
       "       playtime_forever  \n",
       "0              229119.0  \n",
       "1               91837.0  \n",
       "2               56735.0  \n",
       "3               83813.0  \n",
       "4              477522.0  \n",
       "...                 ...  \n",
       "87621            1716.0  \n",
       "87622          432433.0  \n",
       "87623            9361.0  \n",
       "87624           86455.0  \n",
       "87625           21890.0  \n",
       "\n",
       "[87626 rows x 3 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_generos_horas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_x/y99l7l9s60q1726y6701xbkw0000gn/T/ipykernel_81137/3496855238.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  developer_df['items_free_por_anio'] = developer_df['price'].apply(lambda x: 1 if x == 0 else 0)\n"
     ]
    }
   ],
   "source": [
    "#genero un dataframe para consumir en la función developer\n",
    "df_developer= users_games[['developer','price','release_year','item_id_x']]\n",
    "df_developer.to_parquet('df_developer.parquet', engine='pyarrow', compression='snappy')\n",
    "\n",
    " # Filtro el DataFrame original por desarrollador y seleccionar columnas relevantes\n",
    "developer_df = users_games[['developer', 'price', 'release_year', 'item_id_x']]\n",
    "\n",
    "# Calculo la cantidad de ítems gratuitos (Free) por año para todos los desarrolladores\n",
    "developer_df['items_free_por_anio'] = developer_df['price'].apply(lambda x: 1 if x == 0 else 0)\n",
    "items_free_por_anio = developer_df.groupby(['developer', 'release_year'])['items_free_por_anio'].sum().reset_index()\n",
    "\n",
    "# Calculo la cantidad total de ítems por año para todos los desarrolladores\n",
    "items_totales_por_anio = developer_df.groupby(['developer', 'release_year'])['item_id_x'].nunique().reset_index()\n",
    "\n",
    "# Exporto el DataFrame a un archivo Parquet\n",
    "df_developer = items_free_por_anio.merge(items_totales_por_anio, on=['developer', 'release_year'])\n",
    "df_developer.to_parquet('df_developer.parquet', engine='pyarrow', compression='snappy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
