{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IGENIERÍA DE DATOS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INGESTA DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importo la librería pandas y gzip (cabe aclarar que se debe tener instalado pandas previamente, para eso utilizo el comando pip install pandas)\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pyarrow as pa"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decido utilizar los 3 archivos directamente en formato comprimido, para el caso del archivo \"steam_games\", se puede leer directamente utilizando\n",
    "gzip pero para los siguientes 2 archivos: \"user_reviews\" y \"user_items\" hubo que hacer una función ya que el json venía con comillas simples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ingesta y limpieza de datos del primer archivo \"steam_games\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con gzip abrimos el json y generamos un dataframe\n",
    "with gzip.open('/Users/gaston/Documents/Carrera Data Analytics/Proyectos Individuales/Proyecto 1/steam_games.json.gz', 'rt', encoding='utf-8') as steam_games:\n",
    "       \n",
    "    df_steam_games = pd.read_json(steam_games, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decido borrar algunas columnas que no formaban parte del diccionario de datos y que además formaban parte de otro dataset\n",
    "steam_games = df_steam_games.drop(columns = ['items','user_id','steam_id','items_count','tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borro nulos\n",
    "steam_games.dropna(inplace = True, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#realizo una función para separar la columna \"release_date\" en year, month y day para luego consumir en las funciones\n",
    "def procesar_fecha(fecha):\n",
    "    if fecha and fecha != 'none':\n",
    "        partes = fecha.split(\"-\")\n",
    "        if len(partes) == 3:\n",
    "            año = partes[0]\n",
    "            mes = partes[1]\n",
    "            dia = partes[2]\n",
    "            return año, mes, dia\n",
    "    return None, None, None\n",
    "\n",
    "# Aplico la función a la columna \"release_date\" y creo columnas separadas para año, mes y día\n",
    "steam_games[['release_year', 'release_month', 'release_day']] = steam_games['release_date'].apply(lambda x: pd.Series(procesar_fecha(x)))\n",
    "\n",
    "# Elimino la columna original \"release_date\"\n",
    "steam_games.drop(columns=['release_date'], inplace=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ingesta y limpieza de datos del archivo \"user_reviews\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora genero un dataframe para \"user_reviews\" utilizando una función que recorre y completa una lista vacía con la cual genero el DF\n",
    "import ast #importo la librería ast a utilizar\n",
    "\n",
    "info = [] \n",
    "\n",
    "for i in gzip.open('/Users/gaston/Documents/Carrera Data Analytics/Proyectos Individuales/Proyecto 1/user_reviews.json.gz'):\n",
    "     info.append(ast.literal_eval(i.decode('utf-8')))\n",
    "     \n",
    "df_user_reviews = pd.DataFrame(info) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con las funciones explode y normalize desanidamos las columna \"reviews\" para poder\n",
    "# disponibilizar esos datos para consumirlos en etapas posteriores\n",
    "df_reviews_exploded = df_user_reviews.explode('reviews')\n",
    "df_reviews_desanidado = pd.json_normalize(df_reviews_exploded['reviews'].dropna())\n",
    "\n",
    "# Por otra parte reindexamos los DF\n",
    "df_reviews_desanidado.reset_index(inplace=True)\n",
    "df_reviews_exploded.reset_index(inplace=True)\n",
    "\n",
    "# Concatenamos y eliminamosla columna que originalmente desanidamos\n",
    "\n",
    "user_reviews = pd.concat([df_reviews_exploded,df_reviews_desanidado], axis=1)\n",
    "user_reviews = user_reviews.drop(columns = ['reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verifico y borro los duplicados\n",
    "user_reviews_duplicados =  user_reviews.duplicated( keep=\"first\")\n",
    "user_reviews_duplicados.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_reviews = user_reviews.drop_duplicates(keep = \"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#realizo una función para separar la columna \"posted\" en dia, mes y año para luego consumir en las funciones\n",
    "\n",
    "def procesar_fecha(data):\n",
    "    import re\n",
    "    if isinstance(data, str):\n",
    "        match = re.search(r'(\\w+) (\\d+), (\\d+)', data)\n",
    "        if match:\n",
    "            mes = match.group(1)\n",
    "            dia = match.group(2)\n",
    "            año = match.group(3)\n",
    "            meses = {\n",
    "                'January': '01',\n",
    "                'February': '02',\n",
    "                'March': '03',\n",
    "                'April': '04',\n",
    "                'May': '05',\n",
    "                'June': '06',\n",
    "                'July': '07',\n",
    "                'August': '08',\n",
    "                'September': '09',\n",
    "                'October': '10',\n",
    "                'November': '11',\n",
    "                'December': '12'\n",
    "            }\n",
    "            mes_numero = meses.get(mes)\n",
    "            if mes_numero is not None:\n",
    "                return mes_numero, dia, año\n",
    "    return None, None, None\n",
    "\n",
    "# Aplico la función a la columna \"posted\" después de convertirla en cadenas de texto y unifico la fecha en el formato\n",
    "#YYYY-MM-DD para consumir en las funciones\n",
    "user_reviews[['Mes', 'Día', 'Año']] = user_reviews['posted'].astype(str).apply(lambda x: pd.Series(procesar_fecha(x)))\n",
    "user_reviews['Año'].fillna(0, inplace=True)\n",
    "user_reviews['Mes'].fillna(0, inplace=True)\n",
    "user_reviews['Día'].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "# Convierto las columnas 'Año', 'Mes' y 'Día' en enteros\n",
    "user_reviews['Año'] = user_reviews['Año'].astype(int)\n",
    "user_reviews['Año'] = user_reviews['Año'].fillna(0).astype(int)\n",
    "user_reviews['Mes'] = user_reviews['Mes'].astype(int)\n",
    "user_reviews['Mes'] = user_reviews['Mes'].fillna(0).astype(int)\n",
    "user_reviews['Día'] = user_reviews['Día'].astype(int)\n",
    "user_reviews['Día'] = user_reviews['Día'].fillna(0).astype(int)\n",
    "\n",
    "# Defino una fecha predeterminada para las filas con valores cero\n",
    "fecha_predeterminada = pd.to_datetime('1900-01-01')\n",
    "\n",
    "# Crea una nueva columna 'fecha_review' con la fecha predeterminada para las filas con valores cero\n",
    "user_reviews['fecha_review'] = user_reviews.apply(lambda row: pd.to_datetime(f\"{row['Año']}-{row['Mes']}-{row['Día']}\", format='%Y-%m-%d', errors='coerce') if row['Año'] != 0 and row['Mes'] != 0 and row['Día'] != 0 else fecha_predeterminada, axis=1)\n",
    "\n",
    "# Elimino la columna original \"posted\"\n",
    "user_reviews.drop(columns=['posted'], inplace=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sobre este dataset, realizo el análisis de sentimiento con NLP utilizando la librería NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al ejecutar estos comandos, me encontré con errores por los certificados SSL, y tuve que instalar de otra forma como\n",
    "muestro en el siguiente Markdown\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defino la función para analizar los sentimientos\n",
    "def analizar_sentimiento(texto):\n",
    "    if isinstance(texto, str):\n",
    "        sia = SentimentIntensityAnalyzer()\n",
    "        sentimiento = sia.polarity_scores(texto)\n",
    "    \n",
    "        # Determinar la etiqueta en función de la puntuación compuesta\n",
    "        if sentimiento['compound'] >= 0.05:\n",
    "            return '2'\n",
    "        elif sentimiento['compound'] <= -0.05:\n",
    "            return '0'\n",
    "        else:\n",
    "            return '1'\n",
    "    else:\n",
    "            return '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplico la fución sobre la columna \"review\"\" y genero una nueva denominada \"sentimiento\" donde indico con 0 si es negativo\n",
    "# 1 si es neutral y 2 si es positivo de acuerdo a lo solicitado\n",
    "user_reviews['sentimiento'] = user_reviews['review'].apply(analizar_sentimiento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Borro registros con valores nulos en la columna \"review\" y duplicados\n",
    "user_reviews = user_reviews.dropna(subset=['review'])\n",
    "user_reviews = user_reviews.drop_duplicates( keep = \"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elimino la columma \"index\"\n",
    "user_reviews = user_reviews.drop(columns = ['index'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ingesta y limpieza de datos del archivo \"user_items\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora genero un dataframe para \"users_items\" utilizando una función que recorre y completa una lista vacía con la cual genero el DF\n",
    "import ast #importo la librería ast a utilizar\n",
    "\n",
    "info = [] \n",
    "\n",
    "for i in gzip.open('/Users/gaston/Documents/Carrera Data Analytics/Proyectos Individuales/Proyecto 1/users_items.json.gz'):\n",
    "     info.append(ast.literal_eval(i.decode('utf-8')))\n",
    "     \n",
    "df_users_items = pd.DataFrame(info) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con las funciones explode y normalize desanidamos las columna \"items\" para poder\n",
    "# disponibilizar esos datos para consumirlos en etapas posteriores\n",
    "df_items_exploded = df_users_items.explode('items')\n",
    "df_items_desanidado = pd.json_normalize(df_items_exploded['items'])\n",
    "\n",
    "# Por otra parte reindexamos los DF\n",
    "df_items_exploded.reset_index(inplace=True)\n",
    "df_items_desanidado.reset_index(inplace=True)\n",
    "\n",
    "# Concatenamos y eliminamos la columna que previamente desanidamos\n",
    "user_items = pd.concat([df_items_exploded,df_items_desanidado], axis=1)\n",
    "user_items = user_items.drop(columns = ['items'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#identifico valores duplicados, como no hay registros duplicados, no es necesario borrar\n",
    "user_items_duplicados =  user_items.duplicated( keep=\"first\")\n",
    "user_items_duplicados.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#borro los duplicados\n",
    "user_items = user_items.drop_duplicates(keep = \"first\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uno los datasets (DataFrames) user_items & steam_games ya que su info es complementaria y necesaria para las funciones\n",
    "que debemos armar pero primero cambio el tipo de dato a float para poder unirlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_items['item_id']=user_items['item_id'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steam_games['item_id']=user_items['item_id'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_items_steam_games = user_items.merge(steam_games, left_on='item_id',right_on='id',how='left')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la columna precio, defino que si el valor no es numérico, sea Nan para evitar errores de cálculos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_items_steam_games['price'] = user_items_steam_games['price'].apply(lambda x: 0 if isinstance(x, str) else (x if pd.notna(x) else np.nan))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRATAMIENTO DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#borro las columnas \"index\" del dataframe user_items_steam_games\n",
    "user_items_steam_games = user_items_steam_games.drop(columns = ['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Al intentar exportar el df a parquet, encontré que en la columna \"metascore\" tenía valores \"none\" que\n",
    "#me generaban error ya que no es un Nan, por lo que decido reemplazarlos por NA\n",
    "\n",
    "# Reemplazo 'NA' con NaN en la columna 'metascore'\n",
    "user_items_steam_games['metascore'] = user_items_steam_games['metascore'].replace('NA', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decido borrar las columnas playtime_2weeks, specs y publisher para reducir espacio en el dataframe user_items_steam_games\n",
    "user_items_steam_games = user_items_steam_games.drop(columns = ['playtime_2weeks'])\n",
    "user_items_steam_games = user_items_steam_games.drop(columns = ['specs'])\n",
    "user_items_steam_games = user_items_steam_games.drop(columns = ['publisher'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de confeccionados los dataframes con la info que necesito para consumir en las funciones,\n",
    "decido guardarlos en archivos parquet y no en csv para optimizar "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para eso primero instalo la biblioteca pyarrow con el comando pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renombro los dataframes\n",
    "reviews= user_reviews\n",
    "users_games = user_items_steam_games\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En los siguientes pasos, acomodo los Datasets en Dataframes específicos para cada función de la API y poder así luego\n",
    "asegurar el deploy en render."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#genero un archivo parquet con el dataframe \"reviews\" para utilizar en las funcione de la api\n",
    "reviews.to_parquet('user_reviews.parquet', engine='pyarrow', compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#genero un dataframe para consumir en la función userdata\n",
    "df_userdata = users_games[['user_id','price']]\n",
    "df_userdata.to_parquet('df_userdata.parquet', engine='pyarrow', compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#genero un dataframe para consumir en la función genre_ranking\n",
    "users_games_genero = users_games[['genres','playtime_forever']]\n",
    " #Realizo un explode de la columna 'genres' para poder desanidar la información en distintos registros\n",
    "users_games_generos = users_games_genero[['genres','playtime_forever']].explode('genres')\n",
    "    \n",
    "# Agrupo por género y sumo los valores de \"playtime_forever\"\n",
    "agrupado = users_games_generos.groupby('genres')['playtime_forever'].sum().reset_index()\n",
    "\n",
    "# Ordeno por la suma de \"playtime_forever\" en orden descendente\n",
    "agrupado = agrupado.sort_values(by='playtime_forever', ascending=False)\n",
    "agrupado.to_parquet('generos.parquet', engine='pyarrow', compression='snappy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#genero un nuevo dataframe con las columnas genre, playtime_forever, user_id y user_url. Lo exporto a parquet para luego consumirlo\n",
    "df_generos_horas = users_games[['user_id','user_url','genres','playtime_forever']]\n",
    "#Realizo un explode de la columna 'genres' para poder desanidar la información en distintos registros\n",
    "df_generos_horas = df_generos_horas.explode('genres')\n",
    "    \n",
    "# Agrupo por usuario y sumo las horas de juego para cada usuario\n",
    "df_generos_horas = df_generos_horas.groupby(['genres','user_id', 'user_url'])['playtime_forever'].sum().reset_index()\n",
    "\n",
    "df_generos_horas.to_parquet('generos_horas.parquet', engine='pyarrow', compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_generos_horas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_generos_horas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#genero un dataframe para consumir en la función developer\n",
    "df_developer= users_games[['developer','price','release_year','item_id_x']]\n",
    "df_developer.to_parquet('df_developer.parquet', engine='pyarrow', compression='snappy')\n",
    "\n",
    " # Filtro el DataFrame original por desarrollador y seleccionar columnas relevantes\n",
    "developer_df = users_games[['developer', 'price', 'release_year', 'item_id_x']]\n",
    "\n",
    "# Calculo la cantidad de ítems gratuitos (Free) por año para todos los desarrolladores\n",
    "developer_df['items_free_por_anio'] = developer_df['price'].apply(lambda x: 1 if x == 0 else 0)\n",
    "items_free_por_anio = developer_df.groupby(['developer', 'release_year'])['items_free_por_anio'].sum().reset_index()\n",
    "\n",
    "# Calculo la cantidad total de ítems por año para todos los desarrolladores\n",
    "items_totales_por_anio = developer_df.groupby(['developer', 'release_year'])['item_id_x'].nunique().reset_index()\n",
    "\n",
    "# Exporto el DataFrame a un archivo Parquet\n",
    "df_developer = items_free_por_anio.merge(items_totales_por_anio, on=['developer', 'release_year'])\n",
    "df_developer.to_parquet('df_developer.parquet', engine='pyarrow', compression='snappy')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como último paso de este Notebook, preparo el Dataset steam_games para realizar luego el EDA y el modelo de recomendación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ajusto la columna price, defino que si el valor no es numérico, sea 0 para evitar errores de cálculos\n",
    "steam_games['price'] = steam_games['price'].apply(lambda x: 0 if isinstance(x, str) else (x if pd.notna(x) else np.nan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steam_games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "steam_games = steam_games[['genres','price','early_access','id','release_year','publisher','app_name','title','developer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steam_games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporto el DF steam_games para realizar el EDA luego\n",
    "steam_games.to_parquet('steam_games.parquet', engine='pyarrow', compression='snappy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
